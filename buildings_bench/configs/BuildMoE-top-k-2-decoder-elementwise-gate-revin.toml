[model]
context_len = 168
pred_len = 24
max_context_len = 336
max_pred_len = 168
num_decoder_layers = 16
nhead = 12
dim_feedforward = 2048
d_model = 768
dropout = 0.0
activation = 'gelu'
continuous_loads = true
ignore_spatial = false
continuous_head = 'huber'
num_experts = 8
top_k = 2
arch_mode = 'decoder'
n_shared_experts = 0
use_elementwise_gate = true
use_revin = true

[pretrain]
batch_size = 16
init_scale = 0.02
warmup_steps = 20000
lr = 0.00006
train_tokens = 2000000000
eval_every = 5
apply_scaler_transform = 'boxcox'
note = ''

[zero_shot]
batch_size = 2048
apply_scaler_transform = 'boxcox'

[transfer_learning]
apply_scaler_transform = 'boxcox'
