{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to BuildingsBench","text":""},{"location":"#overview","title":"Overview","text":"<p>BuildingsBench is a platform for:</p> <ul> <li>Large-scale pretraining with the synthetic Buildings-900K dataset for short-term load forecasting (STLF). Buildings-900K is statistically representative of the entire U.S. building stock.</li> <li>Benchmarking on two tasks evaluating generalization: zero-shot STLF and transfer learning for STLF.</li> </ul> <p>We provide an index-based PyTorch Dataset for large-scale pretraining, easy data loading for multiple real building energy consumption datasets as PyTorch Tensors or Pandas DataFrames, from simple (persistence) to advanced (transformers) baselines, metrics management, a tokenizer based on KMeans for load time series, and more.</p>"},{"location":"#load-a-benchmark-dataset","title":"Load a benchmark dataset","text":"<pre><code>import torch\nfrom buildings_bench import load_torch_dataset\n# Load a dataset generator for a dataset of buildings\nbuildings_dataset_generator = load_torch_dataset('bdg-2:panther')\n# Each building is a torch.utils.data.Dataset\nfor building_name, building in buildings_dataset_generator:\nbuilding_dataloader = torch.utils.data.DataLoader(building,\nbatch_size=358,\nnum_workers=4,\nshuffle=False)\nfor sample in building_dataloader:\nx = sample['load']\n# context = x[:, :168], 1 week hourly of context\n# target = x[:, -24:], 24 hour target prediction\n# ...\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>To just access the provided dataloaders, models, metrics, etc., install the package with:</p> <pre><code>pip install buildings_bench\n</code></pre> <p>To run the benchmark itself with provided Python scripts, clone this repository and install it in editable mode in a virtual environment or a conda environment.</p> <p>First, create an environment with <code>python&gt;=3.8</code>, for example: <code>conda create -n buildings_bench python=3.8</code>.</p> <p>Then, install the package in editable mode with <pre><code>git clone https://github.com/NREL/BuildingsBench.git\ncd BuildingsBench\npip install -e \".[benchmark]\"\n</code></pre></p>"},{"location":"#installing-faiss-gpu","title":"Installing faiss-gpu","text":"<p>Due to a PyPI limitation, we have to install <code>faiss-gpu</code> (for KMeans) by directly downloading the wheel from https://github.com/kyamagu/faiss-wheels/releases/. Download the wheel for the python version you are using, then install it in your environment.</p> <p>For example:</p> <pre><code>wget https://github.com/kyamagu/faiss-wheels/releases/download/v1.7.3/faiss_gpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\npip install faiss_gpu-1.7.3-cp38-cp38-manylinux2014_x86_64.whl\n</code></pre>"},{"location":"#optional-lightgbm","title":"[Optional] LightGBM","text":"<p>If running the LightGBM baseline, you will need to install LightGBM. Follow instructions here for your OS.  Then, <code>pip install skforecast</code>.</p>"},{"location":"#environment-variables","title":"Environment variables","text":"<p>Set the environment variable <code>BUILDINGS_BENCH</code> to the path where your stored the datasets <code>BuildingsBench</code>.</p> <pre><code>export BUILDINGS_BENCH=/path/to/BuildingsBench`\n</code></pre> <p>If using <code>wandb</code>, set the following:</p> <ul> <li><code>WANDB_ENTITY</code>: your wandb username</li> <li><code>WANDB_PROJECT</code>: the name of your wandb project for this benchmark</li> </ul>"},{"location":"#download-the-datasets","title":"Download the datasets","text":"<p>Download the tar files to disk and untar, which will create a directory called <code>BuildingsBench</code> with the data.</p> <p>The files will be available for download here.</p>"},{"location":"#run-tests","title":"Run tests","text":"<p>Verify your installation by running unit tests:</p> <pre><code>python -m unittest\n</code></pre>"},{"location":"running/","title":"Running The Benchmark","text":""},{"location":"running/#running-the-benchmark","title":"Running The Benchmark","text":"<p>We provide scripts in the <code>./scripts</code> directory for pretraining and to run the benchmark tasks (zero-shot STLF and transfer learning), either with our provided baselines or your own model.</p> <p>Our benchmark assumes each model takes as input a dictionary of torch tensors with the following keys:</p> <pre><code>{\n'load': torch.Tensor,               # (batch_size, seq_len, 1)\n'building_type': torch.LongTensor,  # (batch_size, seq_len, 1)\n'day_of_year': torch.FloatTensor,   # (batch_size, seq_len, 1)\n'hour_of_day': torch.FloatTensor,   # (batch_size, seq_len, 1)\n'day_of_week': torch.FloatTensor,   # (batch_size, seq_len, 1)\n'latitude': torch.FloatTensor,      # (batch_size, seq_len, 1)\n'longitude': torch.FloatTensor,     # (batch_size, seq_len, 1)\n}\n</code></pre> <p>To use these scripts with your model you'll need to register your model with our platform. </p>"},{"location":"running/#registering-your-model","title":"Registering your model","text":"<p>Please see this step-by-step tutorial for a Jupyter Notebook version of the following instructions.</p> <p>Make sure to have installed the benchmark in editable mode: <code>pip install -e .</code></p> <ol> <li>Create a file called <code>your_model.py</code> with your model's implementation, and make your model a subclass of the base model in <code>./buildings_bench/models/base_model.py</code>. Make sure to implement the abstract methods: <code>forward</code>, <code>loss</code>, <code>load_from_checkpoint</code>, <code>predict</code>, <code>unfreeze_and_get_parameters_for_finetuning</code>.</li> <li>Place this file under <code>./buildings_bench/models/your_model.py.</code></li> <li>Import your model class and add your model's name to the <code>model_registry</code> dictionary in <code>./buildings_bench/models/__init__.py</code>.</li> <li>Create a TOML config file under <code>./configs/your_model.toml</code> with each keyword argument your model expects in its constructor (i.e., the hyperparameters for your model) and any additional args for the script you want to run.</li> </ol> <p>The TOML config file should look something like this:</p> <p><pre><code>[model]\n# your model's keyword arguments\n[pretrain]\n# override any of the default pretraining argparse args here\n[zero_shot]\n# override any of the default zero_shot argparse args here\n[transfer_learning]\n# override any of the default transfer_learning argparse args here\n</code></pre> See <code>./configs/TransformerWithTokenizer-L.toml</code> for an example.</p>"},{"location":"running/#pretraining","title":"Pretraining","text":"<p><code>python3 scripts/pretrain.py --config your_model.toml</code></p> <p>This script is implemented with PyTorch <code>DistributedDataParallel</code>, so it can be launched with <code>torchrun</code>. See <code>./scripts/pretrain.sh</code> for an example.</p>"},{"location":"running/#zero-shot-stlf","title":"Zero-shot STLF","text":"<p><code>python3 scripts/zero_shot.py --config your_model.toml --checkpoint /path/to/checkpoint.pt</code></p>"},{"location":"running/#transfer-learning-for-stlf","title":"Transfer Learning for STLF","text":"<p><code>python3 scripts/transfer_learning_torch.py --config your_model.toml --checkpoint /path/to/checkpoint.pt</code> </p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<ul> <li>Dataset Quick Start</li> <li>Register a new model with BuildingsBench</li> <li>Compute aggregate statistics from results files</li> </ul>"},{"location":"API/data/buildings_bench-data/","title":"buildings_bench.data","text":"<p>Functions and class definitions for loading Torch and Pandas datasets.</p> <p>Main entry points for loading PyTorch and Pandas datasets:</p> <ul> <li><code>load_pretraining()</code> (used for pretraining)</li> <li><code>load_torch_dataset()</code> (used for benchmark tasks)</li> <li><code>load_pandas_dataset()</code> (used for benchmark tasks)</li> </ul> <p>Available PyTorch Datasets:</p> <ul> <li><code>Buildings900K</code> (used for pretraining)</li> <li><code>TorchBuildingsDataset</code> (used for benchmark tasks)</li> <li><code>PandasTransformerDataset</code> (used for benchmark tasks)</li> </ul>"},{"location":"API/data/buildings_bench-data/#load_pretraining","title":"load_pretraining","text":""},{"location":"API/data/buildings_bench-data/#buildings_bench.data.load_pretraining","title":"buildings_bench.data.load_pretraining","text":"<pre><code>buildings_bench.data.load_pretraining(name: str, num_buildings_ablation: int = -1, apply_scaler_transform: str = '', scaler_transform_path: Path = None, context_len: Path = 168, pred_len: Path = 24) -&gt; torch.utils.data.Dataset\n</code></pre> <p>Pre-training datasets: buildings-900k-train, buildings-900k-val</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to load.</p> required <code>num_buildings_ablation</code> <code>int</code> <p>Number of buildings to use for pre-training.                             If -1, use all buildings.</p> <code>-1</code> <code>apply_scaler_transform</code> <code>str</code> <p>If not using quantized load or unscaled loads,                      applies a {boxcox,standard} scaling transform to the load. Default: ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to data for transform, e.g., pickled data for BoxCox transform.</p> <code>None</code> <code>context_len</code> <code>int</code> <p>Length of the context. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of the prediction horizon. Defaults to 24.</p> <code>24</code> <p>Returns:</p> Type Description <code>torch.utils.data.Dataset</code> <p>torch.utils.data.Dataset: Dataset for pretraining.</p>"},{"location":"API/data/buildings_bench-data/#load_torch_dataset","title":"load_torch_dataset","text":""},{"location":"API/data/buildings_bench-data/#buildings_bench.data.load_torch_dataset","title":"buildings_bench.data.load_torch_dataset","text":"<pre><code>buildings_bench.data.load_torch_dataset(name: str, dataset_path: Path = None, apply_scaler_transform: str = '', scaler_transform_path: Path = None, context_len: Path = 168, pred_len: Path = 24) -&gt; Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]\n</code></pre> <p>Load datasets by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to load.</p> required <code>dataset_path</code> <code>Path</code> <p>Path to the benchmark data. Optional.</p> <code>None</code> <code>apply_scaler_transform</code> <code>str</code> <p>If not using quantized load or unscaled loads,                      applies a {boxcox,standard} scaling transform to the load. Default: ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to data for transform, e.g., pickled data for BoxCox transform.</p> <code>None</code> <code>context_len</code> <code>int</code> <p>Length of the context. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of the prediction horizon. Defaults to 24.</p> <code>24</code> <p>Returns:</p> Name Type Description <code>dataset</code> <code>Union[TorchBuildingDatasetsFromCSV, TorchBuildingDatasetFromParquet]</code> <p>Dataset for benchmarking.</p>"},{"location":"API/data/buildings_bench-data/#load_pandas_dataset","title":"load_pandas_dataset","text":""},{"location":"API/data/buildings_bench-data/#buildings_bench.data.load_pandas_dataset","title":"buildings_bench.data.load_pandas_dataset","text":"<pre><code>buildings_bench.data.load_pandas_dataset(name: str, dataset_path: Path = None, feature_set: str = 'engineered', apply_scaler_transform: str = '', scaler_transform_path: Path = None) -&gt; PandasBuildingDatasetsFromCSV\n</code></pre> <p>Load datasets by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to load.</p> required <code>dataset_path</code> <code>Path</code> <p>Path to the benchmark data. Optional.</p> <code>None</code> <code>feature_set</code> <code>str</code> <p>Feature set to use. Default: 'engineered'.</p> <code>'engineered'</code> <code>apply_scaler_transform</code> <code>str</code> <p>If not using quantized load or unscaled loads,                         applies a {boxcox,standard} scaling transform to the load. Default: ''. </p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to data for transform, e.g., pickled data for BoxCox transform.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dataset</code> <code>PandasBuildingDatasetsFromCSV</code> <p>Generator of Pandas datasets for benchmarking.</p>"},{"location":"API/data/buildings_bench-data/#the-buildings-900k-pytorch-dataset","title":"The Buildings-900K PyTorch Dataset","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>This is an indexed dataset for the Buildings-900K dataset. It uses an index file to quickly load a sub-sequence from a time series in a multi-building Parquet file. The index file is a tab separated file with the following columns:</p> <ol> <li>Building-type-and-year (e.g., comstock_tmy3_release_1)</li> <li>Census region (e.g., by_puma_midwest)</li> <li>PUMA ID</li> <li>Building ID</li> <li>Hour of year pointer (e.g., 0070)</li> </ol> <p>The sequence pointer is used to extract the slice [pointer - context length : pointer + pred length] for a given building ID.</p> <p>The time series are not stored chronologically and must be sorted by timestamp after loading.</p> <p>Each dataloader worker has its own file pointer to the index file. This is to avoid weird multiprocessing errors from sharing a file pointer. We 'seek' to the correct line in the index file for random access.</p> <p>With 4 workers, data loading with an indexed dataset requires about 30GB of RAM.</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.buildings900K.Buildings900K.__init__","title":"__init__","text":"<pre><code>__init__(dataset_path: Path, index_file: str, context_len: int = 168, pred_len: int = 24, apply_scaler_transform: str = '', scaler_transform_path: Path = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>Path</code> <p>Path to the pretraining dataset.</p> required <code>index_file</code> <code>str</code> <p>Name of the index file</p> required <code>context_len</code> <code>int</code> <p>Length of the context. Defaults to 168.  The index file has to be generated with the same context length.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of the prediction horizon. Defaults to 24. The index file has to be generated with the same pred length.</p> <code>24</code> <code>apply_scaler_transform</code> <code>str</code> <p>Apply a scaler transform to the load. Defaults to ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to the scaler transform. Defaults to None.</p> <code>None</code>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.buildings900K.Buildings900K.init_fp","title":"init_fp","text":"<pre><code>init_fp()\n</code></pre> <p>Each worker needs to open its own file pointer to avoid  weird multiprocessing errors from sharing a file pointer.</p> <p>This is not called in the main process. This is called in the DataLoader worker_init_fn. The file is opened in binary mode which lets us disable buffering.</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.buildings900K.Buildings900K.__read_index_file","title":"__read_index_file","text":"<pre><code>__read_index_file(index_file: Path) -&gt; None\n</code></pre> <p>Extract metadata from index file.</p>"},{"location":"API/data/buildings_bench-data/#torchbuildingdataset","title":"TorchBuildingDataset","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>PyTorch Dataset for a single building's Pandas Dataframe with a timestamp index and a 'power' column.</p> <p>Used to iterate over mini-batches of 192-hour subsequences.</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.TorchBuildingDataset.__init__","title":"__init__","text":"<pre><code>__init__(dataframe: pd.DataFrame, building_latlon: List[float], building_type: BuildingTypes, context_len: int = 168, pred_len: int = 24, sliding_window: int = 24, apply_scaler_transform: str = '', scaler_transform_path: Path = None, is_leap_year: Path = False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>pd.DataFrame</code> <p>Pandas DataFrame with a timestamp index and a 'power' column.</p> required <code>building_latlon</code> <code>List[float]</code> <p>Latitude and longitude of the building.</p> required <code>building_type</code> <code>BuildingTypes</code> <p>Building type for the dataset.</p> required <code>context_len</code> <code>int</code> <p>Length of context. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of prediction. Defaults to 24.</p> <code>24</code> <code>sliding_window</code> <code>int</code> <p>Stride for sliding window to split timeseries into test samples. Defaults to 24.</p> <code>24</code> <code>apply_scaler_transform</code> <code>str</code> <p>Apply scaler transform {boxcox,standard} to the load. Defaults to ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to the pickled data for BoxCox transform. Defaults to None.</p> <code>None</code> <code>is_leap_year</code> <code>bool</code> <p>Is the year a leap year? Defaults to False.</p> <code>False</code>"},{"location":"API/data/buildings_bench-data/#pandastransformerdataset","title":"PandasTransformerDataset","text":"<p>         Bases: <code>torch.utils.data.Dataset</code></p> <p>Create a Torch Dataset out of a Pandas DataFrame.</p> <p>Used to iterate over mini-batches of 192-hour sub-sequences.</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.PandasTransformerDataset.__init__","title":"__init__","text":"<pre><code>__init__(df: pd.DataFrame, context_len: int = 168, pred_len: int = 24, sliding_window: int = 24)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Pandas DataFrame with columns: load, latitude, longitude, hour of day, day of week, day of year, building type</p> required <code>context_len</code> <code>int</code> <p>Length of context.. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of prediction sequence for the forecasting model. Defaults to 24.</p> <code>24</code> <code>sliding_window</code> <code>int</code> <p>Stride for sliding window to split timeseries into test samples. Defaults to 24.</p> <code>24</code>"},{"location":"API/data/buildings_bench-data/#torchbuildingdatasetfromparquet","title":"TorchBuildingDatasetFromParquet","text":"<p>Generate PyTorch Datasets out of Parquet files.</p> <p>Each file has multiple buildings (with same Lat/Lon and building type) and each building is a column. All time series are for the same year.     </p> <p>Attributes:</p> Name Type Description <code>building_datasets</code> <code>dict</code> <p>Maps unique building ids to a TorchBuildingDataset.</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.TorchBuildingDatasetFromParquet.__init__","title":"__init__","text":"<pre><code>__init__(parquet_datasets: List[str], building_latlons: List[List[float]], building_types: List[BuildingTypes], context_len: int = 168, pred_len: int = 24, sliding_window: int = 24, apply_scaler_transform: str = '', scaler_transform_path: Path = None, leap_years: List[int] = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>parquet_datasets</code> <code>List[str]</code> <p>List of paths to a parquet file, each has a timestamp index and multiple columns, one per building.</p> required <code>building_latlons</code> <code>List[List[float]]</code> <p>List of latlons for each parquet file.</p> required <code>building_types</code> <code>List[BuildingTypes]</code> <p>List of building types for each parquet file.</p> required <code>context_len</code> <code>int</code> <p>Length of context. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of prediction. Defaults to 24.</p> <code>24</code> <code>sliding_window</code> <code>int</code> <p>Stride for sliding window to split timeseries into test samples. Defaults to 24.</p> <code>24</code> <code>apply_scaler_transform</code> <code>str</code> <p>Apply scaler transform {boxcox,standard} to the load. Defaults to ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to the pickled data for BoxCox transform. Defaults to None.</p> <code>None</code> <code>leap_years</code> <code>List[int]</code> <p>List of leap years. Defaults to None.</p> <code>None</code>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.TorchBuildingDatasetFromParquet.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Tuple[str, TorchBuildingDataset]]\n</code></pre> <p>Generator to iterate over the building datasets.</p> <p>Yields:</p> Type Description <code>Iterator[Tuple[str, TorchBuildingDataset]]</code> <p>A pair of building id, TorchBuildingDataset objects.</p>"},{"location":"API/data/buildings_bench-data/#torchbuildingdatasetfromcsv","title":"TorchBuildingDatasetFromCSV","text":"<p>TorchBuildingDatasetsFromCSV</p> <p>Generate PyTorch Datasets from a list of CSV files.</p> <p>Attributes:</p> Name Type Description <code>building_datasets</code> <code>dict</code> <p>Maps unique building ids to a list of tuples (year, TorchBuildingDataset).</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.TorchBuildingDatasetsFromCSV.__init__","title":"__init__","text":"<pre><code>__init__(data_path: Path, building_year_files: List[str], building_latlon: List[float], building_type: BuildingTypes, context_len: int = 168, pred_len: int = 24, sliding_window: int = 24, apply_scaler_transform: str = '', scaler_transform_path: Path = None, leap_years: List[int] = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the dataset</p> required <code>building_year_files</code> <code>List[str]</code> <p>List of paths to a csv file, each has a timestamp index and multiple columns, one per building.</p> required <code>building_type</code> <code>BuildingTypes</code> <p>Building type for the dataset.</p> required <code>context_len</code> <code>int</code> <p>Length of context. Defaults to 168.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>Length of prediction sequence for the forecasting model. Defaults to 24.</p> <code>24</code> <code>sliding_window</code> <code>int</code> <p>Stride for sliding window to split timeseries into test samples. Defaults to 24.</p> <code>24</code> <code>apply_scaler_transform</code> <code>str</code> <p>Apply scaler transform {boxcox,standard} to the load. Defaults to ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to the pickled data for BoxCox transform. Defaults to None.</p> <code>None</code> <code>leap_years</code> <code>List[int]</code> <p>List of leap years. Defaults to None.</p> <code>None</code>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.TorchBuildingDatasetsFromCSV.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Tuple[str, torch.utils.data.ConcatDataset]]\n</code></pre> <p>A Generator for TorchBuildingDataset objects.</p> <p>Yields:</p> Type Description <code>Iterator[Tuple[str, torch.utils.data.ConcatDataset]]</code> <p>A tuple of the building id and a ConcatDataset of the TorchBuildingDataset objects for all years.</p>"},{"location":"API/data/buildings_bench-data/#pandasbuildingdatasetsfromcsv","title":"PandasBuildingDatasetsFromCSV","text":"<p>Generate Pandas Dataframes from a list of CSV files.</p> <p>Create a dictionary of building datasets from a list of csv files. Used as a generator to iterate over Pandas Dataframes for each building. The Pandas Dataframe contain all of the years of data for the building.</p> <p>Attributes:</p> Name Type Description <code>building_datasets</code> <code>dict</code> <p>Maps unique building ids to a list of tuples (year, Dataframe).</p>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.PandasBuildingDatasetsFromCSV.__init__","title":"__init__","text":"<pre><code>__init__(data_path: Path, building_year_files: List[str], building_latlon: List[float], building_type: BuildingTypes, features: str = 'transformer', apply_scaler_transform: str = '', scaler_transform_path: Path = None, leap_years: List[int] = [])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the dataset</p> required <code>building_year_files</code> <code>List[str]</code> <p>List of paths to a csv file, each has a timestamp index and multiple columns, one per building.</p> required <code>building_type</code> <code>BuildingTypes</code> <p>Building type for the dataset.</p> required <code>features</code> <code>str</code> <p>Type of features to use. Defaults to 'transformer'. {'transformer','engineered'} 'transformer' features: load, latitude, longitude, hour of day, day of week, day of year, building type 'engineered' features are an expansive list of mainly calendar-based features, useful for traditional ML models.</p> <code>'transformer'</code> <code>apply_scaler_transform</code> <code>str</code> <p>Apply scaler transform {boxcox,standard} to the load. Defaults to ''.</p> <code>''</code> <code>scaler_transform_path</code> <code>Path</code> <p>Path to the pickled data for BoxCox transform. Defaults to None.</p> <code>None</code> <code>leap_years</code> <code>List[int]</code> <p>List of leap years. Defaults to None.</p> <code>[]</code>"},{"location":"API/data/buildings_bench-data/#buildings_bench.data.datasets.PandasBuildingDatasetsFromCSV.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Tuple[str, pd.DataFrame]]\n</code></pre> <p>Generator for iterating over the dataset.</p> <p>Yields:</p> Type Description <code>Iterator[Tuple[str, pd.DataFrame]]</code> <p>A pair of building id and Pandas dataframe.  The dataframe has all years concatenated.</p>"},{"location":"API/models/buildings_bench-models/","title":"buildings_bench.models","text":"<p>Available models:</p> <ul> <li>Encoder-decoder time series transformer</li> <li>Persistence Ensemble (<code>AveragePersistence</code>)</li> <li>Previous Day Persistence (<code>CopyLastDayPersistence</code>)</li> <li>Previous Week Persistence (<code>CopyLastWeekPersistence</code>)</li> <li>Linear regression</li> <li>DLinear</li> </ul> <p>Main entry point for loading a BuildingsBench model is <code>model_factory()</code>.</p>"},{"location":"API/models/buildings_bench-models/#model_factory","title":"model_factory","text":""},{"location":"API/models/buildings_bench-models/#buildings_bench.models.model_factory","title":"buildings_bench.models.model_factory","text":"<pre><code>buildings_bench.models.model_factory(model_name: str, model_args: Dict) -&gt; Tuple[torch.nn.Module, Callable, Callable]\n</code></pre> <p>Instantiate and returns a model for the benchmark.</p> <p>Returns the model itself, the loss function to use, and the predict function.</p> <p>The predict function should return a tuple of two tensors:  (point predictions, prediction distribution parameters) where the distribution parameters may be, e.g., logits, or mean and variance.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>model_args</code> <code>Dict</code> <p>The keyword arguments for the model.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>torch.nn.Module</code> <p>the instantiated model  </p> <code>loss</code> <code>Callable</code> <p>loss function</p> <code>predict</code> <code>Callable</code> <p>predict function</p>"},{"location":"API/models/buildings_bench-models/#basemodel","title":"BaseModel","text":"<p>         Bases: <code>nn.Module</code></p> <p>Base class for all models.</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.__init__","title":"__init__","text":"<pre><code>__init__(context_len, pred_len, continuous_loads)\n</code></pre> <p>Init method for BaseModel.</p> <p>Parameters:</p> Name Type Description Default <code>context_len</code> <code>int</code> <p>length of context window</p> required <code>pred_len</code> <code>int</code> <p>length of prediction window</p> required <code>continuous_loads</code> <code>bool</code> <p>whether to use continuous load values</p> required"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(x: Dict) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Forward pass. </p> Expected keys in x <ul> <li>'load': torch.Tensor of shape (batch_size, seq_len, 1)</li> <li>'building_type': torch.LongTensor of shape (batch_size, seq_len, 1)</li> <li>'day_of_year': torch.FloatTensor of shape (batch_size, seq_len, 1)</li> <li>'hour_of_day': torch.FloatTensor of shape (batch_size, seq_len, 1)</li> <li>'day_of_week': torch.FloatTensor of shape (batch_size, seq_len, 1)</li> <li>'latitude': torch.FloatTensor of shape (batch_size, seq_len, 1)</li> <li>'longitude': torch.FloatTensor of shape (batch_size, seq_len, 1)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict</code> <p>dictionary of input tensors</p> required <p>Returns:</p> Type Description <code>Tuple[torch.Tensor, torch.Tensor]</code> <p>predictions, distribution parameters (Tuple[torch.Tensor, torch.Tensor]): outputs</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.loss","title":"loss  <code>abstractmethod</code>","text":"<pre><code>loss(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>A function for computing the loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>torch.Tensor</code> <p>preds of shape (batch_size, seq_len, 1)</p> required <code>y</code> <code>torch.Tensor</code> <p>targets of shape (batch_size, seq_len, 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>torch.Tensor</code> <p>scalar loss</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(x: Dict) -&gt; Tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>A function for making a forecast on x with the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict</code> <p>dictionary of input tensors</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>torch.Tensor</code> <p>of shape (batch_size, pred_len, 1)</p> <code>distribution_parameters</code> <code>torch.Tensor</code> <p>of shape (batch_size, pred_len, -1)</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.unfreeze_and_get_parameters_for_finetuning","title":"unfreeze_and_get_parameters_for_finetuning  <code>abstractmethod</code>","text":"<pre><code>unfreeze_and_get_parameters_for_finetuning()\n</code></pre> <p>For transfer learning. </p> <ul> <li>Set requires_grad=True for parameters being fine-tuned (if necessary)</li> <li>Return the parameters that should be fine-tuned.</li> </ul>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.base_model.BaseModel.load_from_checkpoint","title":"load_from_checkpoint  <code>abstractmethod</code>","text":"<pre><code>load_from_checkpoint(checkpoint_path: Union[str, Path])\n</code></pre> <p>Describes how to load the model from checkpoint_path.</p>"},{"location":"API/models/buildings_bench-models/#time-series-transformer","title":"Time Series Transformer","text":"<p>         Bases: <code>BaseModel</code></p> <p>An encoder-decoder time series Transformer. Based on PyTorch nn.Transformer.</p> <ul> <li>Uses masking in the decoder to prevent the model from peeking into the future</li> <li>Uses N(0, 0.02) for weight initialization</li> <li>Trains with teacher forcing (i.e. the target is used as the input to the decoder)</li> <li>continuous_loads (True) just predict target values                  (False) categorical over quantized load values</li> </ul>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.LoadForecastingTransformer.__init__","title":"__init__","text":"<pre><code>__init__(context_len: int = 168, pred_len: int = 24, vocab_size: int = 2274, num_encoder_layers: int = 3, num_decoder_layers: int = 3, d_model: int = 256, nhead: int = 8, dim_feedforward: int = 256, dropout: float = 0.0, activation: str = 'gelu', continuous_loads: str = False, continuous_head: str = 'mse', ignore_spatial: str = False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>context_len</code> <code>int</code> <p>length of the input sequence.</p> <code>168</code> <code>pred_len</code> <code>int</code> <p>length of the output sequence.</p> <code>24</code> <code>vocab_size</code> <code>int</code> <p>number of quantized load values in the entire vocabulary.</p> <code>2274</code> <code>num_encoder_layers</code> <code>int</code> <p>number of encoder layers.</p> <code>3</code> <code>num_decoder_layers</code> <code>int</code> <p>number of decoder layers.</p> <code>3</code> <code>d_model</code> <code>int</code> <p>number of expected features in the encoder/decoder inputs.</p> <code>256</code> <code>nhead</code> <code>int</code> <p>number of heads in the multi-head attention models.</p> <code>8</code> <code>dim_feedforward</code> <code>int</code> <p>dimension of the feedforward network model.</p> <code>256</code> <code>dropout</code> <code>float</code> <p>dropout value.</p> <code>0.0</code> <code>activation</code> <code>str</code> <p>the activation function of encoder/decoder intermediate layer, relu or gelu.</p> <code>'gelu'</code> <code>continuous_loads</code> <code>bool</code> <p>whether inputs are continuous/to train the model to predict continuous values.</p> <code>False</code> <code>continuous_head</code> <code>str</code> <p>'mse' or 'gaussian_nll'.</p> <code>'mse'</code> <code>ignore_spatial</code> <code>bool</code> <p>whether to ignore the spatial features.</p> <code>False</code>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.LoadForecastingTransformer.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass of the time series transformer. </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict</code> <p>dictionary of input tensors.</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>torch.Tensor</code> <p>[batch_size, pred_len, vocab_size] if not continuous_loads,                    [batch_size, pred_len, 1] if continuous_loads and continuous_head == 'mse',                     [batch_size, pred_len, 2] if continuous_loads and continuous_head == 'gaussian_nll'.</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.LoadForecastingTransformer.generate_sample","title":"generate_sample","text":"<pre><code>generate_sample(x, temperature = 1.0, greedy = False, num_samples = 1)\n</code></pre> <p>Sample from the conditional distribution.</p> <p>Use output of decoder at each prediction step as input to the next decoder step. Implements greedy decoding and random temperature-controlled sampling.</p> <p>Top-k sampling and nucleus sampling are deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dict</code> <p>dictionary of input tensors</p> required <code>temperature</code> <code>float</code> <p>temperature for sampling</p> <code>1.0</code> <code>greedy</code> <code>bool</code> <p>whether to use greedy decoding</p> <code>False</code> <code>num_samples</code> <code>int</code> <p>number of samples to generate</p> <code>1</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>torch.Tensor</code> <p>of shape [batch_size, pred_len, 1] or shape [batch_size, num_samples, pred_len] if num_samples &gt; 1.</p> <code>distribution_parameters</code> <code>torch.Tensor</code> <p>of shape [batch_size, pred_len, 1]. Not returned if sampling.</p>"},{"location":"API/models/buildings_bench-models/#tokenembedding","title":"TokenEmbedding","text":"<p>         Bases: <code>nn.Module</code></p> <p>Helper Module to convert tensor of input indices into corresponding tensor of token embeddings.</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.TokenEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(vocab_size: int, emb_size: int)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>number of quantized load values in the entire vocabulary.</p> required <code>emb_size</code> <code>int</code> <p>embedding size.</p> required"},{"location":"API/models/buildings_bench-models/#positionalencoding","title":"PositionalEncoding","text":"<p>         Bases: <code>nn.Module</code></p> <p>Helper Module that adds positional encoding to the token embedding to introduce a notion of order within a time-series.</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.PositionalEncoding.__init__","title":"__init__","text":"<pre><code>__init__(emb_size: int, dropout: float, maxlen: int = 500)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>emb_size</code> <code>int</code> <p>embedding size.</p> required <code>dropout</code> <code>float</code> <p>dropout rate.</p> required <code>maxlen</code> <code>int</code> <p>maximum possible length of the incoming time series.</p> <code>500</code>"},{"location":"API/models/buildings_bench-models/#timeseriessinusoidalperiodicembedding","title":"TimeSeriesSinusoidalPeriodicEmbedding","text":"<p>         Bases: <code>nn.Module</code></p> <p>This module produces a sinusoidal periodic embedding for a sequence of values in [-1, +1].</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.TimeSeriesSinusoidalPeriodicEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(embedding_dim: int) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>embedding size.</p> required"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.TimeSeriesSinusoidalPeriodicEmbedding.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p><code>x</code> is expected to be [batch_size, seqlen, 1].</p>"},{"location":"API/models/buildings_bench-models/#zeroembedding","title":"ZeroEmbedding","text":"<p>         Bases: <code>nn.Module</code></p> <p>Outputs zeros of the desired output dim.</p>"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.ZeroEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(embedding_dim: int)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>embedding size.</p> required"},{"location":"API/models/buildings_bench-models/#buildings_bench.models.transformers.ZeroEmbedding.forward","title":"forward","text":"<pre><code>forward(x: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p><code>x</code> is expected to be [batch_size, seqlen, 1].</p>"},{"location":"API/models/buildings_bench-models/#persistence-ensemble","title":"Persistence Ensemble","text":"<p>         Bases: <code>BaseModel</code></p> <p>Predict each hour as the average over each previous day.</p>"},{"location":"API/models/buildings_bench-models/#previous-day-persistence","title":"Previous Day Persistence","text":"<p>         Bases: <code>BaseModel</code></p> <p>Predict each hour as the same hour from the previous day.</p>"},{"location":"API/models/buildings_bench-models/#previous-week-persistence","title":"Previous Week Persistence","text":"<p>         Bases: <code>BaseModel</code></p> <p>Predict each hour as the same hour from the previous week.</p>"},{"location":"API/models/buildings_bench-models/#linear-regression","title":"Linear Regression","text":"<p>         Bases: <code>BaseModel</code></p> <p>Linear regression model that does direct forecasting.</p> <p>It has one weight W and one bias b. The output is computed as y = Wx + b, where W is a matrix of shape [pred_len, context_len].</p>"},{"location":"API/models/buildings_bench-models/#dlinear","title":"DLinear","text":"<p>         Bases: <code>BaseModel</code></p> <p>Decomposition-Linear</p>"},{"location":"API/utilities/buildings_bench-evaluation/","title":"buildings_bench.evaluation","text":"<p>The <code>buildings_bench.evaluation</code> module contains the main functionality for evaluting a model on the benchmark tasks.</p> <p>The <code>buildings_bench.evaluation.managers.DatasetMetricsManager</code> class is the main entry point.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#simple-usage","title":"Simple usage","text":"<pre><code>from buildings_bench import BuildingTypes\nfrom buildings_bench.evaluation.managers import DatasetMetricsManager\n# By default, the DatasetMetricsManager keeps track of NRMSE, NMAE, and NMBE\nmetrics_manager = DatasetMetricsManager()\n# Iterate over the dataset using our building dataset generator\nfor building_name, building_dataset in buildings_datasets_generator:\n# Register a new building with the manager\nmetrics_manager.add_building_to_dataset_if_missing(\ndataset_name, building_name,\n)\n# Your model makes predictions\n# ...\n# Register the predictions with the manager\nmetrics_manager(\ndataset_name,                 # the name of the dataset, e.g., electricity\nbuilding_name,                # the name of the building, e.g., MT_001\ncontinuous_targets,           # the ground truth 24 hour targets\npredictions,                  # the model's 24 hour predictions\nBuildingTypes.RESIDENTIAL_INT,    # an int indicating the building type\n)\n</code></pre>"},{"location":"API/utilities/buildings_bench-evaluation/#advanced-usage-with-scoring-rule","title":"Advanced usage (with scoring rule)","text":"<pre><code>from buildings_bench.evaluation.managers import DatasetMetricsManager\nfrom buildings_bench.evaluation import scoring_rule_factory\nmetrics_manager = DatasetMetricsManager(scoring_rule = scoring_rule_factory('crps'))\n# Iterate over the dataset\nfor building_name, building_dataset in buildings_datasets_generator:\n# Register a new building with the manager\nmetrics_manager.add_building_to_dataset_if_missing(\ndataset_name, building_name,\n)\n# Your model makes predictions\n# ...\n# Register the predictions with the manager\nmetrics_manager(\ndataset_name,           # the name of the dataset, e.g., electricity\nbuilding_name,          # the name of the building, e.g., MT_001\ncontinuous_targets,     # the ground truth 24 hour targets\npredictions,            # the model's 24 hour predictions\nbuilding_types_mask,    # a boolean tensor indicating building type\ny_categories=targets,   # for scoring rules, the ground truth (discrete categories if using tokenization)\ny_distribution_params=distribution_params, # for scoring rules, the distribution parameters\ncentroids=centroids   # for scoring rules with categorical variables, the centroid values\n)\n</code></pre>"},{"location":"API/utilities/buildings_bench-evaluation/#metrics_factory","title":"metrics_factory","text":""},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.metrics_factory","title":"buildings_bench.evaluation.metrics_factory","text":"<pre><code>buildings_bench.evaluation.metrics_factory(name: str, types: List[MetricType] = [MetricType.SCALAR]) -&gt; List[Metric]\n</code></pre> <p>Create a metric from a name. By default, will return a scalar metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>types</code> <code>List[MetricTypes]</code> <p>The types of the metric.         </p> <code>[MetricType.SCALAR]</code> <p>Returns:</p> Name Type Description <code>metrics_list</code> <code>List[Metric]</code> <p>A list of metrics.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#scoring_rule_factory","title":"scoring_rule_factory","text":""},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.scoring_rule_factory","title":"buildings_bench.evaluation.scoring_rule_factory","text":"<pre><code>buildings_bench.evaluation.scoring_rule_factory(name: str) -&gt; ScoringRule\n</code></pre> <p>Create a scoring rule from a name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the scoring rule.</p> required <p>Returns:</p> Name Type Description <code>sr</code> <code>ScoringRule</code> <p>A scoring rule.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#all_metrics_list","title":"all_metrics_list","text":""},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.all_metrics_list","title":"buildings_bench.evaluation.all_metrics_list","text":"<pre><code>buildings_bench.evaluation.all_metrics_list() -&gt; List[Metric]\n</code></pre> <p>Returns all registered metrics.</p> <p>Returns:</p> Name Type Description <code>metrics_list</code> <code>List[Metric]</code> <p>A list of metrics.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildingtypes","title":"BuildingTypes","text":"<p>Enum for supported types of buildings.</p> <p>Attributes:</p> Name Type Description <code>RESIDENTIAL</code> <code>str</code> <p>Residential building type.</p> <code>COMMERCIAL</code> <code>str</code> <p>Commercial building type.</p> <code>RESIDENTIAL_INT</code> <code>int</code> <p>Integer representation of residential building type (0).</p> <code>COMMERCIAL_INT</code> <code>int</code> <p>Integer representation of commercial building type (1).</p>"},{"location":"API/utilities/buildings_bench-evaluation/#datasetmetricsmanager","title":"DatasetMetricsManager","text":"<p>A class that manages a MetricsManager for each building in one or more benchmark datasets.  One DatasetMetricsManager can be used to keep track of all metrics when evaluating a model on all of the benchmark's datasets.</p> <p>This class wil create a Pandas Dataframe summary containing the metrics for each building.</p> <p>Default metrics are NRMSE (CVRMSE), NMAE, NMBE.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.DatasetMetricsManager.summary","title":"summary","text":"<pre><code>summary(dataset_name: str = None) -&gt; pd.DataFrame\n</code></pre> <p>Return a summary of the metrics for the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset to summarize. If None, summarize all datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A Pandas dataframe with the following columns:</p> <ul> <li>dataset: The name of the dataset.</li> <li>building_id: The unique ID of the building.</li> <li>building_type: The type of the building.</li> <li>metric: The name of the metric.</li> <li>metric_type: The type of the metric. (scalar or hour_of_day)</li> <li>value: The value of the metric.</li> </ul>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.DatasetMetricsManager.__call__","title":"__call__","text":"<pre><code>__call__(dataset_name: str, building_id: str, y_true: torch.Tensor, y_pred: torch.Tensor, building_types_mask: torch.Tensor = None, building_type: int = BuildingTypes.COMMERCIAL_INT, **kwargs: int) -&gt; None\n</code></pre> <p>Compute metrics for a batch of predictions for a single building in a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset.</p> required <code>building_id</code> <code>str</code> <p>The unique building identifier.</p> required <code>y_true</code> <code>torch.Tensor</code> <p>The true (unscaled) load values. (continuous) shape is [batch_size, pred_len, 1]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>The predicted (unscaled) load values. (continuous) shape is [batch_size, pred_len, 1]</p> required <code>building_types_mask</code> <code>torch.Tensor</code> <p>A boolean mask indicating the building type of each building. True (1) if commercial, False (0). Shape is [batch_size]. Default is None.</p> <code>None</code> <code>building_type</code> <code>int</code> <p>The building type of the batch. Can be provided  instead of building_types_mask if all buildings are of the same type.</p> <code>BuildingTypes.COMMERCIAL_INT</code> <p>Other Parameters:</p> Name Type Description <code>y_categories</code> <code>torch.Tensor</code> <p>The true load values. (quantized)</p> <code>y_distribution_params</code> <code>torch.Tensor</code> <p>logits, Gaussian params, etc.</p> <code>centroids</code> <code>torch.Tensor</code> <p>The bin values for the quantized load.</p> <code>loss</code> <code>torch.Tensor</code> <p>The loss for the batch.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#metricsmanager","title":"MetricsManager","text":"<p>A class that keeps track of all metrics (and a scoring rule)for one or more buildings.</p> <p>Metrics are computed for each building type (residential and commercial).</p> <p>Example:</p> <pre><code>from buildings_bench.evaluation.managers import MetricsManager\nfrom buildings_bench.evaluation import metrics_factory\nfrom buildings_bench import BuildingTypes\nimport torch\nmetrics_manager = MetricsManager(metrics=metrics_factory('cvrmse'))\nmetrics_manager(\ny_true=torch.FloatTensor([1, 2, 3]).view(1,3,1),\ny_pred=torch.FloatTensor([1, 2, 3]).view(1,3,1),\nbuilding_type = BuildingTypes.RESIDENTIAL_INT\n)\nfor metric in metrics_manager.metrics[BuildingTypes.RESIDENTIAL]:\nmetric.mean()\nprint(metric.value) # prints tensor(0.)\n</code></pre>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.MetricsManager.__init__","title":"__init__","text":"<pre><code>__init__(metrics: List[Metric] = None, scoring_rule: ScoringRule = None)\n</code></pre> <p>Initializes the MetricsManager.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>List[Metric]</code> <p>A list of metrics to compute for each building type.</p> <code>None</code> <code>scoring_rule</code> <code>ScoringRule</code> <p>A scoring rule to compute for each building type.</p> <code>None</code>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.MetricsManager.get_ppl","title":"get_ppl","text":"<pre><code>get_ppl()\n</code></pre> <p>Returns the perplexity of the accumulated loss.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.MetricsManager.summary","title":"summary","text":"<pre><code>summary(with_loss = False, with_ppl = False)\n</code></pre> <p>Return a summary of the metrics for the dataset.</p> <p>A summary maps keys to objects of type Metric or ScoringRule.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.MetricsManager.reset","title":"reset","text":"<pre><code>reset(loss: bool = True) -&gt; None\n</code></pre> <p>Reset the metrics.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.managers.MetricsManager.__call__","title":"__call__","text":"<pre><code>__call__(y_true: torch.Tensor, y_pred: torch.Tensor, building_types_mask: torch.Tensor = None, building_type: int = BuildingTypes.COMMERCIAL_INT, **kwargs: int)\n</code></pre> <p>Compute metrics for a batch of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>The true (unscaled) load values. (continuous) shape is [batch_size, pred_len, 1]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>The predicted (unscaled) load values. (continuous) shape is [batch_size, pred_len, 1]</p> required <code>building_types_mask</code> <code>torch.Tensor</code> <p>A boolean mask indicating the building type of each building. True (1) if commercial, False (0). Shape is [batch_size].</p> <code>None</code> <code>building_type</code> <code>int</code> <p>The building type of the batch. Can be provided  instead of building_types_mask if all buildings are of the same type.</p> <code>BuildingTypes.COMMERCIAL_INT</code> <p>Other Parameters:</p> Name Type Description <code>y_categories</code> <code>torch.Tensor</code> <p>The true load values. (quantized)</p> <code>y_distribution_params</code> <code>torch.Tensor</code> <p>logits, Gaussian params, etc.</p> <code>centroids</code> <code>torch.Tensor</code> <p>The bin values for the quantized load.</p> <code>loss</code> <code>torch.Tensor</code> <p>The loss for the batch.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#metrictype","title":"MetricType","text":"<p>Enum class for metric types.</p> <p>Attributes:</p> Name Type Description <code>SCALAR</code> <code>str</code> <p>A scalar metric.</p> <code>HOUR_OF_DAY</code> <code>str</code> <p>A metric that is calculated for each hour of the day.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildingsbenchmetric","title":"BuildingsBenchMetric","text":"<p>An abstract class for all metrics.</p> <p>The basic idea is to acculumate the errors etc. in a list and then calculate the mean of the errors etc. at the end of the evaluation.</p> <p>Calling the metric will add the error to the list of errors. Calling <code>.mean()</code> will calculate the mean of the errors, populating the <code>.value</code> attribute.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>type</code> <code>MetricType</code> <p>The type of the metric.</p> <code>value</code> <code>float</code> <p>The value of the metric.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#metric","title":"Metric","text":"<p>         Bases: <code>BuildingsBenchMetric</code></p> <p>A class that represents an error metric.  </p> <p>Example:</p> <pre><code>rmse = Metric('rmse', MetricType.SCALAR, squared_error, sqrt=True)\nmae = Metric('mae', MetricType.SCALAR, absolute_error)\nnmae = Metric('nmae', MetricType.SCALAR, absolute_error, normalize=True)\ncvrmse = Metric('cvrmse', MetricType.SCALAR, squared_error, normalize=True, sqrt=True)\nnmbe = Metric('nmbe', MetricType.SCALAR, bias_error, normalize=True)\n</code></pre>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.metrics.Metric.__init__","title":"__init__","text":"<pre><code>__init__(name: str, type: MetricType, function: Callable, **kwargs: Callable)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>type</code> <code>MetricType</code> <p>The type of the metric.</p> required <code>function</code> <code>Callable</code> <p>A function that takes two tensors and returns a tensor.</p> required <p>Other Parameters:</p> Name Type Description <code>normalize</code> <code>bool</code> <p>Whether to normalize the error.</p> <code>sqrt</code> <code>bool</code> <p>Whether to take the square root of the error.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.metrics.Metric.__call__","title":"__call__","text":"<pre><code>__call__(y_true, y_pred) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>shape [batch_size, pred_len]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>shape [batch_size, pred_len]</p> required"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.metrics.Metric.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the metric.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.metrics.Metric.mean","title":"mean","text":"<pre><code>mean() -&gt; None\n</code></pre> <p>Calculate the mean of the error metric.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#absolute_error","title":"absolute_error","text":"<p>A PyTorch method that calculates the absolute error (AE) metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <p>Returns:</p> Name Type Description <code>error</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p>"},{"location":"API/utilities/buildings_bench-evaluation/#squared_error","title":"squared_error","text":"<p>A PyTorch method that calculates the squared error (SE) metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <p>Returns:</p> Name Type Description <code>error</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p>"},{"location":"API/utilities/buildings_bench-evaluation/#bias_error","title":"bias_error","text":"<p>A PyTorch method that calculates the bias error (BE) metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p> required <p>Returns:</p> Name Type Description <code>error</code> <code>torch.Tensor</code> <p>[batch, pred_len]</p>"},{"location":"API/utilities/buildings_bench-evaluation/#scoringrule","title":"ScoringRule","text":"<p>         Bases: <code>BuildingsBenchMetric</code></p> <p>An abstract class for all scoring rules.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#rankedprobabilityscore","title":"RankedProbabilityScore","text":"<p>         Bases: <code>ScoringRule</code></p> <p>A class that calculates the ranked probability score (RPS) metric for categorical distributions.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.scoring_rules.RankedProbabilityScore.rps","title":"rps","text":"<pre><code>rps(y_true, y_pred_logits, centroids) -&gt; None\n</code></pre> <p>A PyTorch method that calculates the ranked probability score metric    for categorical distributions.</p> <p>Since the bin values are centroids of clusters along the real line,    we have to compute the width of the bins by summing the distance to    the left and right centroids of the bin (divided by 2), except for    the first and last bins, where we only need to sum the distance to    the right centroid of the first bin and the left centroid of the    last bin, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>of shape [batch_size, seq_len, 1] categorical labels</p> required <code>y_pred_logits</code> <code>torch.Tensor</code> <p>of shape [batch_size, seq_len, vocab_size] logits</p> required <code>centroids</code> <code>torch.Tensor</code> <p>of shape [vocab_size]</p> required"},{"location":"API/utilities/buildings_bench-evaluation/#continuousrankedprobabilityscore","title":"ContinuousRankedProbabilityScore","text":"<p>         Bases: <code>ScoringRule</code></p> <p>A class that calculates the Gaussian continuous ranked probability score (CRPS) metric.</p>"},{"location":"API/utilities/buildings_bench-evaluation/#buildings_bench.evaluation.scoring_rules.ContinuousRankedProbabilityScore.crps","title":"crps","text":"<pre><code>crps(true_continuous, y_pred_distribution_params) -&gt; None\n</code></pre> <p>Computes the Gaussian CRPS.</p> <p>Parameters:</p> Name Type Description Default <code>true_continuous</code> <code>torch.Tensor</code> <p>of shape [batch_size, seq_len, 1]</p> required <code>y_pred_distribution_params</code> <code>torch.Tensor</code> <p>of shape [batch_size, seq_len, 2]</p> required"},{"location":"API/utilities/buildings_bench-evaluation/#aggregate","title":"aggregate","text":"<p>Compute the aggregate median for a list of models and metrics over all buildings. Also returns the stratified 95% boostrap CIs for the aggregate median.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list</code> <p>List of models to compute aggregate median for.</p> required <code>results_dir</code> <code>str</code> <p>Path to directory containing results.</p> required <code>experiment</code> <code>str</code> <p>Experiment type. Defaults to 'zero_shot'. Options: 'zero_shot', 'transfer_learning'.</p> <code>'zero_shot'</code> <code>metrics</code> <code>list</code> <p>List of metrics to compute aggregate median for. Defaults to ['cvrmse'].</p> <code>['cvrmse']</code> <code>exclude_simulated</code> <code>bool</code> <p>Whether to exclude simulated data. Defaults to True.</p> <code>True</code> <code>only_simulated</code> <code>bool</code> <p>Whether to only include simulated data. Defaults to False.</p> <code>False</code> <code>oov_list</code> <code>list</code> <p>List of OOV buildings to exclude. Defaults to [].</p> <code>[]</code> <code>reps</code> <code>int</code> <p>Number of bootstrap replicates to use. Defaults to 50000.</p> <code>50000</code> <p>Returns:</p> Name Type Description <code>result_dict</code> <code>Dict</code> <p>Dictionary containing aggregate median and CIs for each metric and building type.</p>"},{"location":"API/utilities/buildings_bench-tokenizer/","title":"buildings_bench.tokenizer","text":""},{"location":"API/utilities/buildings_bench-tokenizer/#tokenizer-quick-start","title":"Tokenizer Quick Start","text":""},{"location":"API/utilities/buildings_bench-tokenizer/#instantiate-a-loadquantizer","title":"Instantiate a LoadQuantizer","text":"<pre><code>from buildings_bench.tokenizer import LoadQuantizer\ntransform_path =  Path(os.environ.get('BUILDINGS_BENCH')) / 'metadata' / 'transforms'\nload_transform = LoadQuantizer(\nwith_merge=True,  # Default vocabulary has merged KMeans centroids\nnum_centroids=2274, # Default vocabulary has 2,274 tokens\ndevice='cuda:0' if 'cuda' in args.device else 'cpu')\n# Load the saved faiss KMeans state from disk\nload_transform.load(transform_path)\n</code></pre>"},{"location":"API/utilities/buildings_bench-tokenizer/#quantize-a-load-time-series","title":"Quantize a load time series","text":"<pre><code>batch['load'] = load_transform.transform(batch['load'])\n</code></pre>"},{"location":"API/utilities/buildings_bench-tokenizer/#dequantize-transformer-predictions","title":"Dequantize transformer predictions","text":"<pre><code># predictions are a Tensor of shape [batch_size, pred_len, 1] of quantized values\n# distribution_params is a Tensor of shape [batch_size, pred_len, num_centroids] of logits\npredictions, distribution_params = model.predict(batch)\n# Dequantize the predictions\npredictions = load_transform.undo_transform(predictions)\n</code></pre>"},{"location":"API/utilities/buildings_bench-tokenizer/#extract-the-categorical-distribution","title":"Extract the categorical distribution","text":"<pre><code># First, apply softmax to the logits to normalize them into a categorical distribution\ndistribution_params = torch.softmax(distribution_params, dim=-1)\n# The merged centroid values are the load values corresponding\n# to each token. Note that the merged centroids are already sorted\n# in increasing order.\n# if using merge...\nload_values = load_transform.merged_centroids\n# else, load_values = load_transform.kmeans.centroids.squeeze()\n# Now, distribution_params[i] is the probability \n# assigned to load_values[i].\n</code></pre>"},{"location":"API/utilities/buildings_bench-tokenizer/#loadquantizer","title":"LoadQuantizer","text":"<p>Quantize load timeseries with KMeans. Merge centroids that are within a threshold.</p>"},{"location":"API/utilities/buildings_bench-tokenizer/#buildings_bench.tokenizer.LoadQuantizer.__init__","title":"__init__","text":"<pre><code>__init__(seed: int = 1, num_centroids: int = 2274, with_merge: int = False, merge_threshold: int = 0.01, device: str = 'cpu')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>random seed. Default: 1.</p> <code>1</code> <code>num_centroids</code> <code>int</code> <p>number of centroids: Default: 2274.</p> <code>2274</code> <code>with_merge</code> <code>bool</code> <p>whether to merge centroids that are within a threshold: Default: False.</p> <code>False</code> <code>merge_threshold</code> <code>float</code> <p>threshold for merging centroids. Default: 0.01 (kWh).</p> <code>0.01</code> <code>device</code> <code>str</code> <p>cpu or cuda. Default: cpu.</p> <code>'cpu'</code>"},{"location":"API/utilities/buildings_bench-tokenizer/#buildings_bench.tokenizer.LoadQuantizer.train","title":"train","text":"<pre><code>train(sample: np.ndarray) -&gt; None\n</code></pre> <p>Fit KMeans to a subset of the data. </p> <p>Optionally, merge centroids that are within a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>np.ndarray</code> <p>shape [num_samples, 1]</p> required"},{"location":"API/utilities/buildings_bench-tokenizer/#buildings_bench.tokenizer.LoadQuantizer.transform","title":"transform","text":"<pre><code>transform(sample: Union[np.ndarray, torch.Tensor]) -&gt; Union[np.ndarray, torch.Tensor]\n</code></pre> <p>Quantize a sample of load values into a sequence of indices.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>of shape (n, 1) or (b,n,1).  type is numpy if device is cpu or torch Tensor if device is cuda.</p> required <p>Returns:</p> Name Type Description <code>sample</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>of shape (n, 1) or (b,n,1).</p>"},{"location":"API/utilities/buildings_bench-tokenizer/#buildings_bench.tokenizer.LoadQuantizer.undo_transform","title":"undo_transform","text":"<pre><code>undo_transform(sample: Union[np.ndarray, torch.Tensor]) -&gt; Union[np.ndarray, torch.Tensor]\n</code></pre> <p>Dequantize a sample of integer indices into a sequence of load values.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>of shape (n, 1) or (b,n,1).  type is numpy if device is cpu or torch Tensor if device is cuda.</p> required <p>Returns:</p> Name Type Description <code>sample</code> <code>Union[np.ndarray, torch.Tensor]</code> <p>of shape (n, 1) or (b,n,1).</p>"},{"location":"API/utilities/buildings_bench-transforms/","title":"buildings_bench.transforms","text":""},{"location":"API/utilities/buildings_bench-transforms/#boxcoxtransform","title":"BoxCoxTransform","text":""},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform","title":"buildings_bench.transforms.BoxCoxTransform","text":"<p>A class that computes and applies the Box-Cox transform to data.</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.__init__","title":"__init__","text":"<pre><code>__init__(max_datapoints = 1000000)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_datapoints</code> <code>int</code> <p>If the number of datapoints is greater than this, subsample.</p> <code>1000000</code>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.train","title":"train","text":"<pre><code>train(data: np.array) -&gt; None\n</code></pre> <p>Train the Box-Cox transform on the data with sklearn.preprocessing.PowerTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.array</code> <p>of shape (n, 1) or (b,n,1)</p> required"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.save","title":"save","text":"<pre><code>save(output_path: Path) -&gt; None\n</code></pre> <p>Save the Box-Cox transform</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.load","title":"load","text":"<pre><code>load(saved_path: Path) -&gt; None\n</code></pre> <p>Load the Box-Cox transform</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.transform","title":"transform","text":"<pre><code>transform(sample: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>Transform a sample via Box-Cox. Not ran on the GPU, so input/output are numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>np.ndarray</code> <p>of shape (n, 1) or (b,n,1) </p> required <p>Returns:</p> Name Type Description <code>transformed_sample</code> <code>np.ndarray</code> <p>of shape (n, 1) or (b,n,1)</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.BoxCoxTransform.undo_transform","title":"undo_transform","text":"<pre><code>undo_transform(sample: Union[np.ndarray, torch.Tensor]) -&gt; Union[np.ndarray, torch.Tensor]\n</code></pre> <p>Undo the transformation of a sample via Box-Cox</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>np.ndarray) or (torch.LongTensor</code> <p>of shape (n, 1) or (b,n,1).  numpy if device is cpu or torch Tensor if device is cuda.</p> required <p>Returns:</p> Name Type Description <code>unscaled_sample</code> <code>np.ndarray or torch.Tensor</code> <p>of shape (n, 1) or (b,n,1).</p>"},{"location":"API/utilities/buildings_bench-transforms/#standardscalertransform","title":"StandardScalerTransform","text":""},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform","title":"buildings_bench.transforms.StandardScalerTransform","text":"<p>A class that standardizes data by removing the mean and scaling to unit variance.</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.__init__","title":"__init__","text":"<pre><code>__init__(max_datapoints = 1000000, device = 'cpu')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_datapoints</code> <code>int</code> <p>If the number of datapoints is greater than this, subsample.</p> <code>1000000</code> <code>device</code> <code>str</code> <p>'cpu' or 'cuda'</p> <code>'cpu'</code>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.train","title":"train","text":"<pre><code>train(data: np.array) -&gt; None\n</code></pre> <p>Train the StandardScaler transform on the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.array</code> <p>of shape (n, 1) or (b,n,1)</p> required"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.save","title":"save","text":"<pre><code>save(output_path: Path) -&gt; None\n</code></pre> <p>Save the StandardScaler transform</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.load","title":"load","text":"<pre><code>load(saved_path: Path) -&gt; None\n</code></pre> <p>Load the StandardScaler transform</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.transform","title":"transform","text":"<pre><code>transform(sample: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor\n</code></pre> <p>Transform a sample via StandardScaler</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>np.ndarray or torch.Tensor</code> <p>shape (n, 1) or (b,n,1) </p> required <p>Returns:</p> Name Type Description <code>transformed_samples</code> <code>torch.Tensor</code> <p>shape (n, 1) or (b,n,1)</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.undo_transform","title":"undo_transform","text":"<pre><code>undo_transform(sample: Union[np.ndarray, torch.Tensor]) -&gt; torch.Tensor\n</code></pre> <p>Undo the transformation of a sample via StandardScaler</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>np.ndarray</code> <p>of shape (n, 1) or (b,n,1) or torch.Tensor of shape (n, 1) or (b,n,1)</p> required <p>Returns:</p> Name Type Description <code>unscaled_sample</code> <code>torch.Tensor</code> <p>of shape (n, 1) or (b,n,1)</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.StandardScalerTransform.undo_transform_std","title":"undo_transform_std","text":"<pre><code>undo_transform_std(scaled_std: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Undo transform for standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>scaled_std</code> <code>torch.Tensor</code> <p>of shape (n, 1) or (b,n,1)</p> required <p>Returns:</p> Name Type Description <code>unscaled_std</code> <code>torch.Tensor</code> <p>of shape (n, 1) or (b,n,1)</p>"},{"location":"API/utilities/buildings_bench-transforms/#latlontransform","title":"LatLonTransform","text":""},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.LatLonTransform","title":"buildings_bench.transforms.LatLonTransform","text":"<p>Pre-processing lat,lon data with standard normalization by Buildings-900K training set.</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.LatLonTransform.transform_latlon","title":"transform_latlon","text":"<pre><code>transform_latlon(latlon: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>Transform a raw Lat/Lon sample into a normalized Lat/Lon sample</p> <p>Parameters:</p> Name Type Description Default <code>latlon</code> <code>np.ndarray</code> <p>of shape (2,).</p> required <p>Returns:</p> Name Type Description <code>transformed_latlon</code> <code>np.ndarray</code> <p>of shape (2,).</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.LatLonTransform.undo_transform","title":"undo_transform","text":"<pre><code>undo_transform(normalized_latlon: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>Undo the transformation of a sample</p> <p>Parameters:</p> Name Type Description Default <code>normalized_latlon</code> <code>np.ndarray</code> <p>of shape (n, 2) or (b,n,2).</p> required <p>Returns:</p> Name Type Description <code>unnormalized_latlon</code> <code>np.ndarray</code> <p>of shape (n, 2) or (b,n,2).</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.LatLonTransform.transform","title":"transform","text":"<pre><code>transform(puma_id: str) -&gt; np.ndarray\n</code></pre> <p>Look up a PUMA ID's normalized Lat/Lon centroid.</p> <p>This is used in the Buildings-900K Dataset to look up a lat/lon for each building's PUMA.</p> <p>Parameters:</p> Name Type Description Default <code>puma_id</code> <code>str</code> <p>PUMA ID</p> required <p>Returns:</p> Name Type Description <code>centroid</code> <code>np.ndarray</code> <p>of shape (1,2)</p>"},{"location":"API/utilities/buildings_bench-transforms/#timestamptransform","title":"TimestampTransform","text":""},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.TimestampTransform","title":"buildings_bench.transforms.TimestampTransform","text":"<p>Extract timestamp features from a Pandas timestamp Series.</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.TimestampTransform.__init__","title":"__init__","text":"<pre><code>__init__(is_leap_year: bool = False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>is_leap_year</code> <code>bool</code> <p>Whether the year of the building data is a leap year or not.</p> <code>False</code>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.TimestampTransform.transform","title":"transform","text":"<pre><code>transform(timestamp_series: pd.DataFrame) -&gt; np.ndarray\n</code></pre> <p>Extract timestamp features from a Pandas timestamp Series.</p> <ul> <li>Day of week (0-6)</li> <li>Day of year (0-364)</li> <li>Hour of day (0-23)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>timestamp_series</code> <code>pd.DataFrame</code> <p>of shape (n,) or (b,n)</p> required <p>Returns:</p> Name Type Description <code>time_features</code> <code>np.ndarray</code> <p>of shape (n,3) or (b,n,3)</p>"},{"location":"API/utilities/buildings_bench-transforms/#buildings_bench.transforms.TimestampTransform.undo_transform","title":"undo_transform","text":"<pre><code>undo_transform(time_features: np.ndarray) -&gt; np.ndarray\n</code></pre> <p>Convert normalized time features back to original time features</p> <p>Parameters:</p> Name Type Description Default <code>time_features</code> <code>np.ndarray</code> <p>of shape (n, 3) or (b,n,3)</p> required <p>Returns:</p> Name Type Description <code>unnormalized_time_features</code> <code>np.ndarray</code> <p>of shape (n, 3) or (b,n,3)</p>"},{"location":"API/utilities/buildings_bench-utils/","title":"buildings_bench.utils","text":""},{"location":"API/utilities/buildings_bench-utils/#common-utilities","title":"Common Utilities","text":""},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils","title":"buildings_bench.utils","text":""},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int = 42) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p>"},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils.save_model_checkpoint","title":"save_model_checkpoint","text":"<pre><code>save_model_checkpoint(model, optimizer, scheduler, step, path)\n</code></pre> <p>Save model checkpoint.</p>"},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils.load_model_checkpoint","title":"load_model_checkpoint","text":"<pre><code>load_model_checkpoint(path, model, optimizer, scheduler, local_rank)\n</code></pre> <p>Load model checkpoint.</p>"},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils.worker_init_fn_eulp","title":"worker_init_fn_eulp","text":"<pre><code>worker_init_fn_eulp(worker_id)\n</code></pre> <p>Set random seed for each worker and init file pointer for Buildings-900K dataset workers.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>worker id</p> required"},{"location":"API/utilities/buildings_bench-utils/#buildings_bench.utils.time_features_to_datetime","title":"time_features_to_datetime","text":"<pre><code>time_features_to_datetime(time_features: np.ndarray, year: int) -&gt; np.array\n</code></pre> <p>Convert time features to datetime objects.</p> <p>Parameters:</p> Name Type Description Default <code>time_features</code> <code>np.ndarray</code> <p>Array of time features. [:,0] is day of year, [:,1] is day of week, [:,2] is hour of day.</p> required <code>year</code> <code>int</code> <p>Year to use for datetime objects.</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: Array of datetime objects.</p>"}]}